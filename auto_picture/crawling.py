import os
import glob
import json
import shutil
from icrawler.builtin import GoogleImageCrawler
from PIL import Image

# ì‘ì—… í´ë” & ìµœì¢… ì €ì¥ í´ë”
JSON_FOLDER = "../src/config/organized/GMK"
WORKSPACE_FOLDER = "./workspace"
FINAL_SAVE_FOLDER = "../public/keycap/"

# í•œ ë²ˆì— ë‹¤ìš´ë¡œë“œí•  ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜
CRAWL_MAX_NUM = 10
# ìµœì†Œ í•©ì¹  ì´ë¯¸ì§€ ìˆ˜
MIN_IMAGE_COUNT = 5

def safe_remove(filepath):
    """íŒŒì¼ ì‚­ì œ (ì˜ˆì™¸ ë°©ì§€)"""
    try:
        if os.path.exists(filepath):
            os.remove(filepath)
            print(f"ğŸ—‘ ì‚­ì œë¨: {filepath}")
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì‚­ì œ ì˜¤ë¥˜: {filepath} ({e})")

def extract_labels_from_json():
    """JSON íŒŒì¼ì—ì„œ (jsonê²½ë¡œ, manufacturer, label) ëª©ë¡ ë½‘ì•„ì˜¤ê¸°."""
    labels = []
    for json_file in glob.glob(os.path.join(JSON_FOLDER, '*.json')):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            if 'label' in data and 'manufacturer' in data:
                manufacturer = data['manufacturer']
                label = data['label'].replace("mkg_", "")
                labels.append((json_file, manufacturer, label))
        except Exception as e:
            print(f"âŒ JSON ì½ê¸° ì˜¤ë¥˜: {json_file} ({e})")
    return labels

def google_image_search(search_query, temp_dir):
    """
    ê²€ìƒ‰ì–´ search_queryë¡œ Google í¬ë¡¤ë§.
    í•œ ë²ˆì— CRAWL_MAX_NUMì¥ ì‹œë„.
    """
    google_crawler = GoogleImageCrawler(storage={"root_dir": temp_dir})
    google_crawler.crawl(
        keyword=search_query,
        filters={"size": "large"},
        max_num=CRAWL_MAX_NUM
    )

def process_downloaded_images_in_order(temp_dir):
    """
    temp_dir ì•ˆ ì´ë¯¸ì§€ë¥¼ **ë‹¤ìš´ë¡œë“œ ìˆœ**(íŒŒì¼ëª… 000001, 000002...)ëŒ€ë¡œ í™•ì¸.
    - ë†’ì´ < 600 ìŠ¤í‚µ (ì‚­ì œ)
    - ì„¸ë¡œ 600 ë¦¬ì‚¬ì´ì¦ˆ â†’ WebP ì €ì¥ í›„ ì›ë³¸ ì‚­ì œ
    - ìˆœì„œëŒ€ë¡œ ìœ íš¨ ì´ë¯¸ì§€ ê²½ë¡œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    """
    # iCrawlerëŠ” ê¸°ë³¸ì ìœ¼ë¡œ íŒŒì¼ëª…ì„ 000001, 000002.. ì´ëŸ° ì‹ìœ¼ë¡œ ì €ì¥í•œë‹¤.
    # glob í›„ì— ì •ë ¬í•´ì„œ ìˆœì„œ ì§€í‚¨ë‹¤.
    filepaths = sorted(glob.glob(os.path.join(temp_dir, "*")))

    valid_paths = []
    for filepath in filepaths:
        if os.path.isdir(filepath):
            # metadataë‚˜ logs í´ë”ëŠ” íŒ¨ìŠ¤
            continue
        try:
            with Image.open(filepath) as img:
                img.load()
                w, h = img.size

                if h < 600:
                    safe_remove(filepath)
                    continue

                # ë¦¬ì‚¬ì´ì¦ˆ (ì„¸ë¡œ 600)
                scale = 600 / h
                new_w = int(w * scale)
                img_resized = img.resize((new_w, 600), Image.LANCZOS)

                base_name, _ = os.path.splitext(os.path.basename(filepath))
                webp_path = os.path.join(temp_dir, f"{base_name}.webp")
                img_resized.save(webp_path, "WEBP", quality=80, optimize=True)

                # ì›ë³¸ ì‚­ì œ
                safe_remove(filepath)

                # ìˆœì„œëŒ€ë¡œ ëˆ„ì 
                valid_paths.append(webp_path)

        except Exception as e:
            print(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {filepath} - {e}")
            safe_remove(filepath)

    return valid_paths

def merge_images_into_scrollable(image_files, output_filepath):
    """
    ì´ë¯¸ì§€ë“¤ì„ ê°€ë¡œë¡œ ë³‘í•©í•˜ì—¬ WebPë¡œ ì €ì¥.
    """
    if len(image_files) < MIN_IMAGE_COUNT:
        print(f"âŒ ìµœì†Œ {MIN_IMAGE_COUNT}ê°œ í•„ìš”í•˜ì§€ë§Œ, {len(image_files)}ê°œë§Œ ìˆìŒ. ë¨¸ì§€ ì·¨ì†Œ.")
        return None

    loaded_images = []
    for path in image_files:
        if os.path.exists(path):
            loaded_images.append(Image.open(path).convert("RGBA"))

    if not loaded_images:
        return None

    total_width = sum(img.width for img in loaded_images)
    max_height = max(img.height for img in loaded_images)

    if total_width == 0 or max_height == 0:
        print("âŒ ë³‘í•© ì‹¤íŒ¨: ì´ë¯¸ì§€ í¬ê¸°ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ.")
        return None

    merged = Image.new("RGBA", (total_width, max_height), (0, 0, 0, 0))
    x_offset = 0
    for img in loaded_images:
        merged.paste(img, (x_offset, 0), mask=img)
        x_offset += img.width
        img.close()

    try:
        merged.save(output_filepath, "WEBP", quality=80, lossless=True)
        print(f"âœ… ë¨¸ì§€ ì™„ë£Œ: {output_filepath}")
        return output_filepath
    except Exception as e:
        print(f"âŒ WebP ì €ì¥ ì˜¤ë¥˜: {e}")
        return None

def process_labels():
    """
    1) JSONì—ì„œ (manufacturer, label) ì½ìŒ
    2) labelë¡œ 50ì¥ í¬ë¡¤ë§
    3) (height<600) ìŠ¤í‚µ + WebP ë³€í™˜ (êµ¬ê¸€ ë‹¤ìš´ë¡œë“œ ìˆœì„œ ìœ ì§€)
    4) ì²« 5ì¥ë§Œ í•©ì³ì„œ ë³‘í•©
    5) ì‚¬ìš©ìì—ê²Œ ì—´ëŒì‹œí‚¨ í›„ 'n' ëˆ„ë¥´ë©´ ì‚­ì œ, ì•„ë‹ˆë©´ ìµœì¢…í´ë”ì— ì €ì¥
    """
    labels = extract_labels_from_json()
    if not labels:
        print("âœ… ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŒ.")
        return

    os.makedirs(WORKSPACE_FOLDER, exist_ok=True)
    os.makedirs(FINAL_SAVE_FOLDER, exist_ok=True)

    for json_file, manufacturer, label in labels:
        out_name = os.path.splitext(os.path.basename(json_file))[0] + ".webp"
        final_path = os.path.join(FINAL_SAVE_FOLDER, out_name)

        # ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì´ ìˆìœ¼ë©´ ìŠ¤í‚µ
        if os.path.exists(final_path):
            print(f"âœ… ì´ë¯¸ ì²˜ë¦¬ë¨: {final_path}")
            continue

        temp_dir = os.path.join(WORKSPACE_FOLDER, label)
        os.makedirs(temp_dir, exist_ok=True)

        # 1) í¬ë¡¤ë§
        search_query = f"{manufacturer} {label}"
        print(f"\n[í¬ë¡¤ë§] {search_query} -> ìµœëŒ€ {CRAWL_MAX_NUM}ì¥")
        google_image_search(search_query, temp_dir)

        # 2) ë‹¤ìš´ë¡œë“œ ìˆœì„œëŒ€ë¡œ í•„í„°ë§ & WebP ë³€í™˜
        valid_paths = process_downloaded_images_in_order(temp_dir)

        # 3) ì•ì—ì„œ 5ì¥ë§Œ ì‚¬ìš©
        top5 = valid_paths[:5]
        if len(top5) < MIN_IMAGE_COUNT:
            print(f"ğŸš« ìœ íš¨ ì´ë¯¸ì§€ê°€ 5ì¥ ë¯¸ë§Œ. ìŠ¤í‚µ: {label}")
            shutil.rmtree(temp_dir, ignore_errors=True)
            continue

        # 4) ë³‘í•©
        merged_out = os.path.join(temp_dir, "merged.webp")
        merged_result = merge_images_into_scrollable(top5, merged_out)
        if not merged_result:
            print(f"ğŸš« ë¨¸ì§€ ì‹¤íŒ¨ ë˜ëŠ” ì˜¤ë¥˜: {label}")
            shutil.rmtree(temp_dir, ignore_errors=True)
            continue

        # 5) ì‚¬ìš©ì í™•ì¸ í›„ ìµœì¢… í´ë”ì— ì €ì¥ or ì‚­ì œ
        try:
            merged_img = Image.open(merged_result)
            merged_img.show()  # ì‹œìŠ¤í…œ ë·°ì–´ì—ì„œ ì—´ê¸°
        except Exception as e:
            print(f"ì´ë¯¸ì§€ ì—´ê¸° ì˜¤ë¥˜: {e}")

        shutil.move(merged_result, final_path)
        print(f"ğŸš€ ìµœì¢… í´ë”ì— ì €ì¥: {final_path}")

        # ì„ì‹œ í´ë” ì‚­ì œ
        shutil.rmtree(temp_dir, ignore_errors=True)

    print("âœ… ëª¨ë“  JSON ì²˜ë¦¬ ë.")

if __name__ == '__main__':
    process_labels()
