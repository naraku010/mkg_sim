import os
import glob
import shutil
import json
import re
from icrawler.builtin import GoogleImageCrawler
from PIL import Image

# ì‘ì—… í´ë” & ìµœì¢… ì €ì¥ í´ë”
JSON_FOLDER = "../src/config/organized/GMK"
WORKSPACE_FOLDER = "./workspace"
FINAL_SAVE_FOLDER = "../public/keycap/"

MIN_IMAGE_COUNT = 5
MAX_ITERATIONS = 5

def safe_remove(filepath):
    """íŒŒì¼ ì‚­ì œ (ì˜ˆì™¸ ë°©ì§€)"""
    try:
        if os.path.exists(filepath):
            os.remove(filepath)
            print(f"ğŸ—‘ ì‚­ì œë¨: {filepath}")
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì‚­ì œ ì˜¤ë¥˜: {filepath} ({e})")

def is_rendered_image(img):
    """ë Œë”ë§ ì´ë¯¸ì§€ ì—¬ë¶€ í™•ì¸ (EXIF ì •ë³´ ì—†ëŠ” ê²½ìš° ë Œë”ë§ ì´ë¯¸ì§€ë¡œ ê°„ì£¼)"""
    try:
        exif = img._getexif()
    except Exception:
        exif = None
    return exif is None

def process_images_from_folder(temp_folder, valid_images, remaining_needed):
    """ë‹¤ìš´ë¡œë“œëœ ì´ë¯¸ì§€ë¥¼ WebPë¡œ ë³€í™˜"""
    count_added = 0
    for filepath in glob.glob(os.path.join(temp_folder, '*')):
        if count_added >= remaining_needed:
            break
        try:
            with Image.open(filepath) as img:
                img.load()
                width, height = img.size

                # í•´ìƒë„ ì¡°ê±´ í™•ì¸ (ë†’ì´ 600px ì´ìƒë§Œ ì‚¬ìš©)
                if height < 600:
                    print(f"âš ï¸ {filepath} - í•´ìƒë„ ë¶€ì¡± ({height}px)")
                    safe_remove(filepath)
                    continue

                # ë Œë”ë§ ì´ë¯¸ì§€ íŒë³„
                if not is_rendered_image(img):
                    print(f"âš ï¸ {filepath} - ë Œë”ë§ ì´ë¯¸ì§€ ì•„ë‹˜")
                    safe_remove(filepath)
                    continue

                # ê°€ë¡œ 800pxë¡œ ë¦¬ì‚¬ì´ì¦ˆ (ë†’ì´ ê¸°ì¤€)
                scale = 600 / height
                img_resized = img.resize((int(width * scale), 600), Image.LANCZOS)

                # WebPë¡œ ì €ì¥
                base_name = os.path.splitext(os.path.basename(filepath))[0]
                final_filepath = os.path.join(temp_folder, base_name + '.webp')
                img_resized.save(final_filepath, 'WEBP', quality=75, optimize=True)

                # ì €ì¥ í™•ì¸
                if os.path.exists(final_filepath):
                    valid_images.append(final_filepath)
                    count_added += 1
                    print(f"âœ… ë³€í™˜ë¨: {final_filepath}")
                else:
                    print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {final_filepath}")

                # ì›ë³¸ ì‚­ì œ
                safe_remove(filepath)

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {filepath} ({e})")
            safe_remove(filepath)
    return count_added

def merge_images_into_scrollable(image_files, output_filepath):
    """ì´ë¯¸ì§€ë“¤ì„ ê°€ë¡œë¡œ ë³‘í•©í•˜ì—¬ íˆ¬ëª… ë°°ê²½ WebP ìƒì„±"""
    image_files = [img for img in image_files if os.path.exists(img)]  # ì¡´ì¬í•˜ëŠ” íŒŒì¼ë§Œ ì‚¬ìš©
    if len(image_files) < MIN_IMAGE_COUNT:
        print(f"âŒ ìµœì†Œ {MIN_IMAGE_COUNT}ê°œ í•„ìš”í•˜ì§€ë§Œ, {len(image_files)}ê°œë§Œ ìˆìŒ. ë³‘í•© ì·¨ì†Œ.")
        return None

    images = [Image.open(img).convert("RGBA") for img in image_files]  # RGBA ë³€í™˜
    total_width = sum(img.width for img in images)
    max_height = max(img.height for img in images)

    if total_width == 0 or max_height == 0:
        print("âŒ ë³‘í•© ì‹¤íŒ¨: ì´ë¯¸ì§€ í¬ê¸°ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ.")
        return None

    # íˆ¬ëª… ë°°ê²½ ìƒì„±
    merged_image = Image.new('RGBA', (total_width, max_height), (0, 0, 0, 0))

    x_offset = 0
    for img in images:
        merged_image.paste(img, (x_offset, 0), mask=img)
        x_offset += img.width
        img.close()

    try:
        merged_image.save(output_filepath, 'WEBP', quality=75, lossless=True)
        print(f"âœ… '{output_filepath}' (íˆ¬ëª… ë°°ê²½) ìƒì„± ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ WebP ì €ì¥ ì˜¤ë¥˜: {e}")
        return None

    return output_filepath

def extract_labels_from_json():
    """JSON íŒŒì¼ì—ì„œ manufacturerì™€ label ê°’ ì¶”ì¶œ (mkg_ ì œê±°)"""
    labels = []
    for json_file in glob.glob(os.path.join(JSON_FOLDER, '*.json')):
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                if 'label' in data and 'manufacturer' in data:
                    manufacturer = data['manufacturer']
                    label = data['label'].replace("mkg_", "")
                    labels.append((json_file, manufacturer, label))
        except Exception as e:
            print(f"âŒ JSON ì½ê¸° ì˜¤ë¥˜: {json_file} ({e})")
    return labels

def process_labels():
    """JSONë³„ manufacturerì™€ labelì„ ê²€ìƒ‰í•˜ì—¬ ì´ë¯¸ì§€ í¬ë¡¤ë§ ë° ë³‘í•©"""
    labels = extract_labels_from_json()
    if not labels:
        print("âŒ JSON íŒŒì¼ì—ì„œ ê²€ìƒ‰í•  labelì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    os.makedirs(WORKSPACE_FOLDER, exist_ok=True)
    os.makedirs(FINAL_SAVE_FOLDER, exist_ok=True)

    for json_file, manufacturer, label in labels:
        search_query = f"{manufacturer} {label}"
        print(f"\nğŸ” '{search_query}' ê²€ìƒ‰ ì¤‘...")

        temp_dir = os.path.join(WORKSPACE_FOLDER, label)
        os.makedirs(temp_dir, exist_ok=True)

        valid_images = []
        iteration = 0

        while len(valid_images) < MIN_IMAGE_COUNT and iteration < MAX_ITERATIONS:
            iteration += 1
            remaining_needed = MIN_IMAGE_COUNT - len(valid_images)

            print(f"[Iteration {iteration}] '{search_query}' ì¶”ê°€ ë‹¤ìš´ë¡œë“œ ({remaining_needed}ê°œ í•„ìš”)...")

            google_crawler = GoogleImageCrawler(storage={'root_dir': temp_dir})
            google_crawler.crawl(keyword=search_query, max_num=remaining_needed * 3)

            added = process_images_from_folder(temp_dir, valid_images, remaining_needed)
            print(f"âœ… {added}ê°œ ì¶”ê°€ë¨ (í˜„ì¬ {len(valid_images)}/{MIN_IMAGE_COUNT})")

        if len(valid_images) < MIN_IMAGE_COUNT:
            print(f"âŒ '{search_query}' ê²€ìƒ‰ ê²°ê³¼ ë¶€ì¡± (ìµœì†Œ {MIN_IMAGE_COUNT}ê°œ í•„ìš”, {len(valid_images)}ê°œ í™•ë³´)")
            shutil.rmtree(temp_dir, ignore_errors=True)
            continue

        json_filename = os.path.splitext(os.path.basename(json_file))[0] + ".webp"
        merged_image_path = os.path.join(temp_dir, json_filename)
        merged_image = merge_images_into_scrollable(valid_images, merged_image_path)

        if merged_image:
            final_destination = os.path.join(FINAL_SAVE_FOLDER, json_filename)
            shutil.move(merged_image, final_destination)
            print(f"ğŸš€ '{final_destination}' ì´ë™ ì™„ë£Œ")

        shutil.rmtree(temp_dir, ignore_errors=True)

    print("âœ… ëª¨ë“  JSON ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == '__main__':
    process_labels()
